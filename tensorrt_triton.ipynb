{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 11 07:15:47 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   39C    P0    25W /  70W |   1117MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     27587      C   tritonserver                     1113MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Jun__8_16:49:14_PDT_2022\n",
      "Cuda compilation tools, release 11.7, V11.7.99\n",
      "Build cuda_11.7.r11.7/compiler.31442593_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Triton Inference Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Nvidia image is at `nvcr.io/nvidia/tritonserver:22.10-py3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the docker image for Triton server, execute the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "docker run --rm -it \\\n",
    " -v $(pwd):/workshop \\\n",
    "  --name triton \\\n",
    "  -p 8888:8888 \\\n",
    "  --runtime=nvidia nvcr.io/nvidia/tritonserver:22.10-py3 \\\n",
    "  tritonserver \\\n",
    "  --backend-config=tensorflow,version=2 \\\n",
    "  --model-repository=/workshop/models \\\n",
    "  --exit-on-error=false \\\n",
    "  --repository-poll-secs=20 \\\n",
    "  --model-control-mode=\"poll\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triton Client for Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install the client for Triton, run the following commands\n",
    "\n",
    "```\n",
    "pip install nvidia-pyindex\n",
    "pip install tritonclient[all]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nvidia-pyindex\n",
      "  Downloading nvidia-pyindex-1.0.9.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: nvidia-pyindex\n",
      "  Building wheel for nvidia-pyindex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvidia-pyindex: filename=nvidia_pyindex-1.0.9-py3-none-any.whl size=8418 sha256=9c7311423f22c89175c77a834afde701180fe577cef02604198e16e061d411bf\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/0e/62/68/8bb6aafc3cb47e3468055aebc10d004b55da43563d748aac9c\n",
      "Successfully built nvidia-pyindex\n",
      "Installing collected packages: nvidia-pyindex\n",
      "Successfully installed nvidia-pyindex-1.0.9\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting tritonclient[all]\n",
      "  Downloading tritonclient-2.27.0-py3-none-manylinux1_x86_64.whl (11.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m219.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from tritonclient[all]) (1.23.4)\n",
      "Collecting python-rapidjson>=0.9.1\n",
      "  Downloading python_rapidjson-1.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m292.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting grpcio==1.41.0\n",
      "  Downloading grpcio-1.41.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m331.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting protobuf<3.20,>=3.5.0\n",
      "  Downloading protobuf-3.19.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m330.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting geventhttpclient<=2.0.2,>=1.4.4\n",
      "  Downloading geventhttpclient-2.0.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (100 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 kB\u001b[0m \u001b[31m265.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp>=3.8.1\n",
      "  Downloading aiohttp-3.8.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m312.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.2 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from grpcio==1.41.0->tritonclient[all]) (1.16.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m284.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m264.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m191.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from aiohttp>=3.8.1->tritonclient[all]) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from aiohttp>=3.8.1->tritonclient[all]) (22.1.0)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting gevent>=0.13\n",
      "  Downloading gevent-22.10.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m262.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting brotli\n",
      "  Downloading Brotli-1.0.9-cp39-cp39-manylinux1_x86_64.whl (357 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.2/357.2 kB\u001b[0m \u001b[31m288.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]) (2022.9.24)\n",
      "Collecting zope.event\n",
      "  Downloading zope.event-4.5.0-py2.py3-none-any.whl (6.8 kB)\n",
      "Collecting zope.interface\n",
      "  Downloading zope.interface-5.5.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (257 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.9/257.9 kB\u001b[0m \u001b[31m172.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting greenlet>=2.0.0\n",
      "  Downloading greenlet-2.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (535 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m535.9/535.9 kB\u001b[0m \u001b[31m304.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from gevent>=0.13->geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]) (65.5.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.8.1->tritonclient[all]) (3.4)\n",
      "Installing collected packages: brotli, zope.interface, zope.event, python-rapidjson, protobuf, multidict, grpcio, greenlet, frozenlist, async-timeout, yarl, tritonclient, gevent, aiosignal, geventhttpclient, aiohttp\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.2\n",
      "    Uninstalling protobuf-3.20.2:\n",
      "      Successfully uninstalled protobuf-3.20.2\n",
      "Successfully installed aiohttp-3.8.3 aiosignal-1.3.1 async-timeout-4.0.2 brotli-1.0.9 frozenlist-1.3.3 gevent-22.10.2 geventhttpclient-2.0.2 greenlet-2.0.1 grpcio-1.41.0 multidict-6.0.2 protobuf-3.19.6 python-rapidjson-1.9 tritonclient-2.27.0 yarl-1.8.1 zope.event-4.5.0 zope.interface-5.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nvidia-pyindex\n",
    "!pip install tritonclient[all]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the Triton Server is ready or not!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 127.0.0.1:8000...\n",
      "* TCP_NODELAY set\n",
      "* Connected to localhost (127.0.0.1) port 8000 (#0)\n",
      "> GET /v2/health/ready HTTP/1.1\n",
      "> Host: localhost:8000\n",
      "> User-Agent: curl/7.68.0\n",
      "> Accept: */*\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\n",
      "< Content-Length: 0\n",
      "< Content-Type: text/plain\n",
      "< \n",
      "* Connection #0 to host localhost left intact\n"
     ]
    }
   ],
   "source": [
    "!curl -v localhost:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Torchscript Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import os\n",
    "# from os.path import join, dirname\n",
    "\n",
    "# torch.cuda.set_per_process_memory_fraction(0.4, 0)\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# folder = \"/workshop/models/pt/1\"\n",
    "# os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# model = torch.nn.Module(...)\n",
    "# sample_img = torch.zeros([1, 3, 32, 32], dtype=torch.float32)\n",
    "# traced_model = torch.jit.trace(model.eval(), sample_img, strict=True)\n",
    "# traced_model.save(join(folder, \"model.pt\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Configuration for the PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"pt\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size : 0\n",
    "input [\n",
    "  {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 32, 32 ]\n",
    "    reshape { shape: [ 1, 3, 32, 32 ] }\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 10 ]\n",
    "    reshape { shape: [ 10 ] }\n",
    "  }\n",
    "]\n",
    "parameters: {\n",
    "key: \"INFERENCE_MODE\"\n",
    "    value: {\n",
    "    string_value: \"true\"\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "with open('/home/ubuntu/work/models/pt/config.pbtxt', 'w') as file:\n",
    "    file.write(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get information regarding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 127.0.0.1:8000...\n",
      "* TCP_NODELAY set\n",
      "* Connected to localhost (127.0.0.1) port 8000 (#0)\n",
      "> GET /v2/models/pt HTTP/1.1\n",
      "> Host: localhost:8000\n",
      "> User-Agent: curl/7.68.0\n",
      "> Accept: */*\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\n",
      "< Content-Type: application/json\n",
      "< Content-Length: 191\n",
      "< \n",
      "* Connection #0 to host localhost left intact\n",
      "{\"name\":\"pt\",\"versions\":[\"1\"],\"platform\":\"pytorch_libtorch\",\"inputs\":[{\"name\":\"input__0\",\"datatype\":\"FP32\",\"shape\":[3,32,32]}],\"outputs\":[{\"name\":\"output__0\",\"datatype\":\"FP32\",\"shape\":[10]}]}"
     ]
    }
   ],
   "source": [
    "!curl -v localhost:8000/v2/models/pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Inference Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as tritonhttpclient\n",
    "VERBOSE = False\n",
    "model_label = 'input__0'\n",
    "input_shape = ( 3, 32, 32)\n",
    "input_dtype = 'FP32'\n",
    "output_name = 'output__0'\n",
    "model_name = 'pt'\n",
    "url = 'localhost:8000'\n",
    "model_version = '1'\n",
    "\n",
    "triton_client = tritonhttpclient.InferenceServerClient(url=url, verbose=VERBOSE)\n",
    "model_metadata = triton_client.get_model_metadata(model_name=model_name, model_version=model_version)\n",
    "model_config = triton_client.get_model_config(model_name=model_name, model_version=model_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# preprocessing function\n",
    "def img_preprocess(img_path=\"../GTC/img/cat.jpg\"):\n",
    "    img = Image.open(img_path)\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2471, 0.2435, 0.2616]),\n",
    "    ])\n",
    "    return preprocess(img).numpy()\n",
    "\n",
    "transformed_img = img_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "input0 = tritonhttpclient.InferInput(model_label, transformed_img.shape, datatype=\"FP32\")\n",
    "input0.set_data_from_numpy(transformed_img, binary_data=False)\n",
    "\n",
    "output = tritonhttpclient.InferRequestedOutput(output_name, binary_data=False, class_count=10)\n",
    "response = triton_client.infer(model_name, model_version=model_version, \n",
    "                               inputs=[input0], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['4.948598:3', '0.938896:5', '0.684368:7', '-0.359170:4',\n",
       "       '-2.540236:2', '-3.929959:6', '-6.445944:0', '-6.604012:9',\n",
       "       '-7.473110:8', '-8.512840:1'], dtype=object)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_label = response.as_numpy(output_name)\n",
    "output_label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has returned the logits in a `logit:class` format. To convert it to a dictionary, we take the following steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../GTC/cifar10_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for r in output_label:\n",
    "    cat = int(r.split(\":\")[1])\n",
    "    conf = float(r.split(\":\")[0])\n",
    "    results[categories[cat]] = conf\n",
    "logits = list(results.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "logits = torch.tensor(logits)\n",
    "preds = (F.softmax(logits, dim=-1) * 100).numpy()\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c,k in enumerate(results):\n",
    "    results[k] = preds[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': 96.3503,\n",
       " 'dog': 1.7476794,\n",
       " 'horse': 1.3549448,\n",
       " 'deer': 0.47722018,\n",
       " 'bird': 0.05388822,\n",
       " 'frog': 0.013425939,\n",
       " 'airplane': 0.0010845922,\n",
       " 'truck': 0.0009260152,\n",
       " 'ship': 0.00038830572,\n",
       " 'automobile': 0.00013728552}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorRT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert it to TensorRT.\n",
    "For this, we may use the PyTorch container @ NGC located at `nvcr.io/nvidia/pytorch:22.10-py3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can (try to) install the pip packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch-tensorrt==1.2.0 --find-links https://github.com/pytorch/TensorRT/releases/expanded_assets/v1.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch_tensorrt/lib/libtorchtrt.so: undefined symbol: _ZN2at4_ops4view4callERKNS_6TensorEN3c108ArrayRefIlEE",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch_tensorrt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m torch\u001b[39m.\u001b[39mhub\u001b[39m.\u001b[39m_validate_not_a_forked_repo\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m a,b,c: \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# load model\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch_tensorrt/__init__.py:85\u001b[0m\n\u001b[1;32m     81\u001b[0m             ctypes\u001b[39m.\u001b[39mCDLL(_find_lib(lib, LINUX_PATHS))\n\u001b[1;32m     83\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_tensorrt\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_compile\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     86\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_tensorrt\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_util\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     87\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_tensorrt\u001b[39;00m \u001b[39mimport\u001b[39;00m ts\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch_tensorrt/_compile.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m List, Dict, Any\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_tensorrt\u001b[39;00m \u001b[39mimport\u001b[39;00m _enums\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch_tensorrt\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mts\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_tensorrt\u001b[39;00m \u001b[39mimport\u001b[39;00m logging\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch_tensorrt/_enums.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_tensorrt\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_C\u001b[39;00m \u001b[39mimport\u001b[39;00m dtype, DeviceType, EngineCapability, TensorFormat\n",
      "\u001b[0;31mImportError\u001b[0m: /opt/conda/envs/pytorch/lib/python3.9/site-packages/torch_tensorrt/lib/libtorchtrt.so: undefined symbol: _ZN2at4_ops4view4callERKNS_6TensorEN3c108ArrayRefIlEE"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_tensorrt\n",
    "torch.hub._validate_not_a_forked_repo=lambda a,b,c: True\n",
    "\n",
    "# load model\n",
    "model = torch.jit.load(\"cifar10-script.pt\")\n",
    "\n",
    "# Compile with Torch TensorRT;\n",
    "trt_model = torch_tensorrt.compile(model,\n",
    "    inputs= [torch_tensorrt.Input((1, 3, 32, 32))],\n",
    "    enabled_precisions= { torch.half} # Run with FP32\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "torch.jit.save(trt_model, \"model1.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"trt\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size : 0\n",
    "input [\n",
    "  {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 32, 32 ]\n",
    "    reshape { shape: [ 1, 3, 32, 32 ] }\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 10 ]\n",
    "    reshape { shape: [ 10 ] }\n",
    "  }\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "with open('/home/ubuntu/work/models/trt/config.pbtxt', 'w') as file:\n",
    "    file.write(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 127.0.0.1:8000...\n",
      "* TCP_NODELAY set\n",
      "* Connected to localhost (127.0.0.1) port 8000 (#0)\n",
      "> GET /v2/models/trt HTTP/1.1\n",
      "> Host: localhost:8000\n",
      "> User-Agent: curl/7.68.0\n",
      "> Accept: */*\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\n",
      "< Content-Type: application/json\n",
      "< Content-Length: 192\n",
      "< \n",
      "* Connection #0 to host localhost left intact\n",
      "{\"name\":\"trt\",\"versions\":[\"1\"],\"platform\":\"pytorch_libtorch\",\"inputs\":[{\"name\":\"input__0\",\"datatype\":\"FP32\",\"shape\":[3,32,32]}],\"outputs\":[{\"name\":\"output__0\",\"datatype\":\"FP32\",\"shape\":[10]}]}"
     ]
    }
   ],
   "source": [
    "!curl -v localhost:8000/v2/models/trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# preprocessing function\n",
    "def img_preprocess(img_path=\"../GTC/img/cat.jpg\"):\n",
    "    img = Image.open(img_path)\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2471, 0.2435, 0.2616]),\n",
    "    ])\n",
    "    return preprocess(img).numpy()\n",
    "\n",
    "transformed_img = img_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as tritonhttpclient\n",
    "VERBOSE = False\n",
    "model_label = 'input__0'\n",
    "input_shape = ( 3, 32, 32)\n",
    "input_dtype = 'FP32'\n",
    "output_name = 'output__0'\n",
    "model_name = 'trt'\n",
    "url = 'localhost:8000'\n",
    "model_version = '1'\n",
    "\n",
    "triton_client = tritonhttpclient.InferenceServerClient(url=url, verbose=VERBOSE)\n",
    "model_metadata = triton_client.get_model_metadata(model_name=model_name, model_version=model_version)\n",
    "model_config = triton_client.get_model_config(model_name=model_name, model_version=model_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "input0 = tritonhttpclient.InferInput(model_label, transformed_img.shape, datatype=\"FP32\")\n",
    "input0.set_data_from_numpy(transformed_img, binary_data=False)\n",
    "\n",
    "output = tritonhttpclient.InferRequestedOutput(output_name, binary_data=False, class_count=10)\n",
    "response = triton_client.infer(model_name, model_version=model_version, \n",
    "                               inputs=[input0], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['4.949219:3', '0.936035:5', '0.686035:7', '-0.360596:4',\n",
       "       '-2.539062:2', '-3.929688:6', '-6.449219:0', '-6.601562:9',\n",
       "       '-7.472656:8', '-8.515625:1'], dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_label = response.as_numpy(output_name)\n",
    "output_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for r in output_label:\n",
    "    cat = int(r.split(\":\")[1])\n",
    "    conf = float(r.split(\":\")[0])\n",
    "    results[categories[cat]] = conf\n",
    "logits = list(results.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.6355713e+01, 1.7417017e+00, 1.3564386e+00, 4.7627077e-01,\n",
       "       5.3921040e-02, 1.3421994e-02, 1.0804347e-03, 9.2776271e-04,\n",
       "       3.8826271e-04, 1.3682646e-04], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "logits = torch.tensor(logits)\n",
    "preds = (F.softmax(logits, dim=-1) * 100).numpy()\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c,k in enumerate(results):\n",
    "    results[k] = preds[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': 96.35571,\n",
       " 'dog': 1.7417017,\n",
       " 'horse': 1.3564386,\n",
       " 'deer': 0.47627077,\n",
       " 'bird': 0.05392104,\n",
       " 'frog': 0.013421994,\n",
       " 'airplane': 0.0010804347,\n",
       " 'truck': 0.0009277627,\n",
       " 'ship': 0.0003882627,\n",
       " 'automobile': 0.00013682646}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
